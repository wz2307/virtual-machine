{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 - Segmenting Chinese text\n",
    "\n",
    "Typically the first task in Natural Language Processing (NLP) is segmentation - splitting a piece of text into a set of words. This is difficult to do with Chinese because while a single word may be composed of one, two, or three characters, the text is written without spaces. To accomplish this non-trivial task we will use a dedicated Python library called [Jieba](https://github.com/fxsjy/jieba). \n",
    "\n",
    "The following code gives examples from [Jieba's github repository](https://github.com/fxsjy/jieba) for segmenting Chinese text into individual words. Once the text is segmented you should be able to use many of the NLP tools provided in the [NLTK library](http://www.nltk.org/) and described in the [NLTK book](http://www.nltk.org/book/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start by importing the jieba library\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_1 = ('在\"\"城中村\"\"改造过程中将涉及到政府、村民和开发商三者的利益冲 突与协调,\"\"城中村\"\"改造合理的制度安排将平衡利益主体之间的关系,从动态博弈理论的分析中可得出三种制度...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 在/ / / / 城中/ 城中村/ 中村/ / / / 改造/ 过程/ 过程中将/ 中将/ 涉及/ 到/ 政府/ / / 村民/ 民和/ 开发/ 开发商/ 三者/ 的/ 利益/ 冲/ / / 突/ 与/ 协调/ / / / / 城中/ 城中村/ 中村/ / / / 改造/ 合理/ 的/ 制度/ 安排/ 将/ 平衡/ 利益/ 主体/ 之间/ 的/ 关系/ / / 从/ 动态/ 博弈/ 理论/ 的/ 分析/ 中/ 可得/ 得出/ 三种/ 制度/ / / / \n",
      "Default Mode: 在/ \"/ \"/ 城中村/ \"/ \"/ 改造/ 过程中将/ 涉及/ 到/ 政府/ 、/ 村民/ 和/ 开发商/ 三者/ 的/ 利益/ 冲/  / 突与/ 协调/ ,/ \"/ \"/ 城中村/ \"/ \"/ 改造/ 合理/ 的/ 制度/ 安排/ 将/ 平衡/ 利益/ 主体/ 之间/ 的/ 关系/ ,/ 从/ 动态/ 博弈/ 理论/ 的/ 分析/ 中/ 可/ 得出/ 三种/ 制度/ ...\n",
      "他, 来到, 了, 网易, 杭研, 大厦\n",
      "小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(para_1, cut_all=True)\n",
    "print(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式\n",
    "\n",
    "seg_list = jieba.cut(para_1, cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))  # 默认模式\n",
    "\n",
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\")\n",
    "print(\", \".join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\")  # 搜索引擎模式\n",
    "print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '来到', '北京', '清华大学']\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False)\n",
    "print(list(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 永和\t\t start: 0 \t\t end:2\n",
      "word 服装\t\t start: 2 \t\t end:4\n",
      "word 饰品\t\t start: 4 \t\t end:6\n",
      "word 有限公司\t\t start: 6 \t\t end:10\n"
     ]
    }
   ],
   "source": [
    "result = jieba.tokenize(u'永和服装饰品有限公司')\n",
    "for tk in result:\n",
    "    print(\"word %s\\t\\t start: %d \\t\\t end:%d\" % (tk[0],tk[1],tk[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
